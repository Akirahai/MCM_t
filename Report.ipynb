{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import *\n",
    "from model import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(\"perf_results.csv\")\n",
    "M = matches['match_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = matches[['match_id','point_no','Alpha 1', 'Alpha 2']].loc[(matches['match_id'] == M[20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(M)\n",
    "train_matches = M[:int(0.9*len(M))]\n",
    "val_matches = M[int(0.9*len(M)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset object for each point in match\n",
    "class Tennis_Dataset(Dataset):\n",
    "    def __init__(self, phase,  path_db, path_lb, matchs):\n",
    "        super(Tennis_Dataset, self).__init__()\n",
    "        self.phase = phase\n",
    "        \n",
    "        lb = pd.read_csv(path_lb)\n",
    "        lb = lb[lb['match_id'].isin(matchs)]\n",
    "        \n",
    "        self.path_db = path_db\n",
    "        self.result = lb\n",
    "        self.output = ['Alpha 1', 'Alpha 2']\n",
    "        \n",
    "        self.json_data = {}\n",
    "        for match_id in matchs:\n",
    "            json_file_path = os.path.join(self.path_db, f\"{match_id}.json\")\n",
    "            with open(json_file_path, \"r\") as file:\n",
    "                self.json_data[match_id] = json.load(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.result.iloc[idx]\n",
    "        id, point_no = row['match_id'], row['point_no']\n",
    "        point_no = f'point_{point_no}'\n",
    "        \n",
    "        data = self.json_data[id]\n",
    "        point_data = data[point_no]\n",
    "        \n",
    "        A = np.array(list(data[point_no].values())).T\n",
    "        \n",
    "        result = np.zeros((400,81))\n",
    "        result[:A.shape[0],:A.shape[1]] = A\n",
    "        \n",
    "        labels = row[self.output].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        return torch.from_numpy(result).float(), torch.from_numpy(labels).float()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Tennis_Dataset('val', 'data', \"perf_results.csv\", val_matches)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= 3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utlis import Train, Test, static\n",
    "from model import Regression_sequential_data, resnet_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_acc(file,net,val_loader,criterion,device):\n",
    "    data=[]      \n",
    "    net.load_state_dict(torch.load(path+file, map_location=device))\n",
    "    size= static.get_size(path+file ,'mb')\n",
    "    \n",
    "    start = 1\n",
    "    RMSE_loss = Test.test_model(val_loader, net, criterion, device)\n",
    "    stop = 2\n",
    "    time=stop - start\n",
    "    data = [file.split(\".\")[0],RMSE_loss,size,time]\n",
    "            \n",
    "    # SNR,AF,IAVB,LBBB,RBBB,STD        \n",
    "    col_names = [\"Model\", \"RMSE_loss\",'Size(MB)','Time(s)']\n",
    "    return data,col_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GRU.plt', 'RNN.plt', 'Resnet.plt', 'LSTM.plt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "path += '/result/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_model = {'RNN.plt': Regression_sequential_data.RegressionRNN(num_sensors= 81, hidden_units=16).to(device),\n",
    "             'GRU.plt': Regression_sequential_data.RegressionGRU(num_sensors= 81, hidden_units=16).to(device),\n",
    "             'LSTM.plt': Regression_sequential_data.RegressionLSTM(num_sensors= 81, hidden_units=16).to(device),\n",
    "             'Resnet.plt': resnet_1d.ResNet18(num_classes=2, channels=81).to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU.plt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.013295840683935262\n",
      "RNN.plt\n",
      "Test loss: 0.013301118469607385\n",
      "Resnet.plt\n",
      "LSTM.plt\n",
      "Test loss: 0.013337185754733914\n"
     ]
    }
   ],
   "source": [
    "data_report = []\n",
    "for file in os.listdir(path):\n",
    "    print(file)\n",
    "    if file == 'Resnet.plt':\n",
    "        continue    \n",
    "    net = dic_model[file]\n",
    "    data, col_names = table_acc(file,net,val_dataloader,criterion,device)\n",
    "    data_report.append(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤═════════╤═════════════╤════════════╤═══════════╕\n",
      "│    │ Model   │   RMSE_loss │   Size(MB) │   Time(s) │\n",
      "╞════╪═════════╪═════════════╪════════════╪═══════════╡\n",
      "│  0 │ GRU     │   0.0132958 │      0.026 │         1 │\n",
      "├────┼─────────┼─────────────┼────────────┼───────────┤\n",
      "│  1 │ RNN     │   0.0133011 │      0.01  │         1 │\n",
      "├────┼─────────┼─────────────┼────────────┼───────────┤\n",
      "│  2 │ LSTM    │   0.0133372 │      0.034 │         1 │\n",
      "╘════╧═════════╧═════════════╧════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(data_report, headers=col_names, tablefmt=\"fancy_grid\", showindex=\"always\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤═════════╤═════════════╤════════════╤═══════════╕\n",
      "│    │ Model   │   RMSE_loss │   Size(MB) │   Time(s) │\n",
      "╞════╪═════════╪═════════════╪════════════╪═══════════╡\n",
      "│  0 │ GRU     │   0.0133388 │      0.026 │         1 │\n",
      "├────┼─────────┼─────────────┼────────────┼───────────┤\n",
      "│  1 │ RNN     │   0.0133163 │      0.01  │         1 │\n",
      "├────┼─────────┼─────────────┼────────────┼───────────┤\n",
      "│  2 │ LSTM    │   0.0133372 │      0.034 │         1 │\n",
      "╘════╧═════════╧═════════════╧════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(data_report, headers=col_names, tablefmt=\"fancy_grid\", showindex=\"always\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
